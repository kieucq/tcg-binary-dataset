Installing environment
1. conda init
2. source ~/.bashrc
3. git clone https://github.com/beekill95/TC-Prediction.git
4. conda env create -f environment.yml -n tc
5. conda activate tc
6. install poetry: curl -sSL https://install.python-poetry.org | python3 -
7. source ~/.bashrc, or export PATH=/N/u/ckieu/Carbonate/.local/bin:$PATH
8. poetry install

Running scripts
1. ./ncep/extract_13vars_grib2.py.py: extract grib2 data for a given domain/vars. See script for input arguments.
   ex: python extract_ncep.py /N/slate/ckieu/tmp/input/ /N/slate/ckieu/tmp/output/test --lat 0 20 --lon 100 120
2. ./ncep/create_labels_v2.py: extract best track to create a list of TC centers and stored in a csv file
   ex: python create_labels_v2.py --best-track /N/project/pfec_climo/qmnguyen/tc_prediction/data/bdeck/ibtracs.ALL.list.v04r00.csv 
       --observations-dir /N/slate/ckieu/tmp/output/test/ --leadtime 0
3. ./ncep/split_train_test_v1.py: generate the training, validation, and test datasets.
   ex: python split_train_test_v1.py --test-from 20210918 --val-from 20210801 --labels /N/slate/ckieu/tmp/output/test/tc_0h.csv
4. ./ncep/create_ncep_binary_stormcenter.py: use TC bestract to generate a set of binary dataset centered on TC genesis location
   with a given domain size. The pos/neg pairs will be the same as the number of TCG events;
5. ./ncep/create_ncep_binary_largedomain.py: use CSV files to generate a set of positive folder for each lead time that contains
   a fixed domain on the date where TCG occurs. All other times will be considered as a negative set. This is highly UNBALANCED
   dataset for training.   

Modifications
1. Changing extracted input variables: modify variable list in extract_environment_features_from_ncep.py. 
   See an example: extract_environment_features_from_ncep_chanh.py
2. Extract dataset from WRF output: modify extract_features_from_theanh_data_baseline.sh, and run this shell script.
3. Create own neg/pos script from Quan's cvs files: see the script create_tc_binary_ncep_chanh.ipynb. 
   The output from this can be fed into Chanh's deeplearning model
4. Visualize Integrated Gradient: login GPU node, deactivate conda base, load deeplearning, conda activate tc, sh jupiter_gpudeeplearning.sh.  
5. Visualize Unet: should be directly extract all output from the unet.py to plot. Note that tensorflow dataset requires some specific 
   way to extract the data/label before you can view it. 
6. Visualize F1 scores: ???
7. Add a new var: check the new implementation sample here: extract_environment_features_from_ncep_newvar.py. need to modify the model 
   resnet.py as well to include a new variables. See an example code resnet_newvar.py for how to add new vars
8. To create binary data files for neg/pos classificaiton with a fix domain size of 30x30 instead of a large domain as in the above scripts,
   run the following script 
   python create_tc_binary_classification_ncep.py --best-track /N/project/pfec_climo/qmnguyen/tc_prediction/data/bdeck/ibtracs.ALL.list.v04r00.csv 
   --ncep-fnl /N/slate/ckieu/tmp/input/ --basin WP --leadtime 0 --domain-size 20 --distance 45 --output /N/slate/ckieu/tmp/output/ncep_binary 


Remarks:
1. The current verison works only with the NCEP/FNL dataset (e.g., fnl_20210701_00_00.grib2)
2. IbTracs best data CSV format is required, which must be of the form "ibtracs.ALL.list.v04r00.csv"
   (see the lastest one here /N/project/pfec_climo/ckieu/data/tc/ibtracs.ALL.list.v04r00.csv)
3. to open a .py file as a Jupyter notebook, first deactivate base when first login to a note, then conda activate tc, then run 
   "sh jupiter_gpudeeplearning.sh h1 9999". cd 


